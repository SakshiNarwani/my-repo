{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telecom Churn Case Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Neccessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from collections import Counter\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maximizing the number of rows and columns to be displayed\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the dataframe\n",
    "data = pd.read_csv('telecom_churn_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Looking for missing values\n",
    "data.isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at the statistical summary\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at the different types of columns\n",
    "date_columns =  ['last_date_of_month_6','last_date_of_month_7','last_date_of_month_8','last_date_of_month_9','date_of_last_rech_6','date_of_last_rech_7','date_of_last_rech_8','date_of_last_rech_9','date_of_last_rech_data_6','date_of_last_rech_data_7','date_of_last_rech_data_8','date_of_last_rech_data_9']\n",
    "categorical_columns = ['night_pck_user_6','night_pck_user_7','night_pck_user_8','night_pck_user_9','fb_user_6','fb_user_7','fb_user_8','fb_user_9']\n",
    "id_columns = ['mobile_number', 'circle_id']\n",
    "numerical_columns = [column for column in data.columns if column not in id_columns + date_columns + categorical_columns]\n",
    "\n",
    "print('Id Columns :',len(id_columns))\n",
    "print('Categorical Columns :',len(categorical_columns))\n",
    "print('Numerical Columns :',len(numerical_columns))\n",
    "print('Date Columns :',len(date_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handling recharge columns\n",
    "rech_col = [i for i in data.columns if 'rech' in i]\n",
    "data[rech_col].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking for missing recharge columns\n",
    "data[rech_col].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Since recharge date and recharge amount are missing which means customer didn't recharge.\n",
    "data.loc[data.total_rech_data_6.isnull() & data.date_of_last_rech_data_6.isnull(), [\"total_rech_data_6\", \"date_of_last_rech_data_6\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of recharge columns where we will impute missing values with zero\n",
    "zero_impute = ['total_rech_data_6', 'total_rech_data_7', 'total_rech_data_8', 'total_rech_data_9',\n",
    "         'av_rech_amt_data_6', 'av_rech_amt_data_7', 'av_rech_amt_data_8', 'av_rech_amt_data_9',\n",
    "         'max_rech_data_6', 'max_rech_data_7', 'max_rech_data_8', 'max_rech_data_9']\n",
    "data[zero_impute] = data[zero_impute].apply(lambda x:x.fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Looking at summary statistics of the zero imputed columns\n",
    "data[zero_impute].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping date and Id columns as they won't be useful in our analysis\n",
    "print('Before dropping :',data.shape)\n",
    "data = data.drop(id_columns + date_columns,axis=1)\n",
    "data.head()\n",
    "print('After dropping :',data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a new Category where missing values are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking for unique count for categorical columns\n",
    "for i in categorical_columns:\n",
    "    print(data[i].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing missing categorical column values with -1\n",
    "data[categorical_columns] = data[categorical_columns].apply(lambda x:x.fillna('-1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for missing values\n",
    "print(data[categorical_columns].isnull().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values present in the categorical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping columns having more than 65% missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at missing value percentage by columns\n",
    "round((data.isnull().sum()/len(data.index))*100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the columns with more than 65% null values as it won't benefit us \n",
    "\n",
    "cols_with_high_missing = data.columns[data.isnull().sum()/len(data.index) > 0.65]\n",
    "print('Before dropping : ',len(data.columns))\n",
    "print('Columns to be dropped : ',len(cols_with_high_missing))\n",
    "data = data.drop(cols_with_high_missing,axis=1)\n",
    "print('After dropping : ',len(data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_with_less_na = [i for i in data.columns if data[i].isnull().mean() < 0.09 and data[i].isnull().mean() > 0]\n",
    "\n",
    "for i in col_with_less_na:\n",
    "    if i not in date_columns:\n",
    "         data[i].fillna(data[i].median(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for missing nulls\n",
    "round((data.isnull().sum()/len(data.index))*100,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now rid of missing values!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Filtering High values customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating total Data Recharge amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['total_data_rech_6'] = data.total_rech_data_6 * data.av_rech_amt_data_6\n",
    "data['total_data_rech_7'] = data.total_rech_data_7 * data.av_rech_amt_data_7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating total Recharge amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['amt_data_6'] = data.total_rech_amt_6 + data.total_data_rech_6\n",
    "data['amt_data_7'] = data.total_rech_amt_7 + data.total_data_rech_7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average amount recharge done by each customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['av_amt_data_6_7'] = (data.total_rech_amt_6 + data.total_rech_amt_7)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the 70th percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Recharge amount at 70th percentile:',data['av_amt_data_6_7'].quantile(0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retaining only customers who have recharges their mobiles with more than or equal to 70th percentile amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_exclude_70 = data.loc[data.av_amt_data_6_7 >= data.av_amt_data_6_7.quantile(0.7), :]\n",
    "data_exclude_70 = data_exclude_70.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting unwanted columns\n",
    "data = data.drop(['total_data_rech_6', 'total_data_rech_7','amt_data_6', 'amt_data_7', 'av_amt_data_6_7'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Derive Churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating total incoming and outgoing minutes of usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_exclude_70['tot_inc_out_9'] = data_exclude_70.total_ic_mou_9 + data_exclude_70.total_og_mou_9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating 2g and 3g data consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_exclude_70['tot_internet_9'] = data_exclude_70.vol_2g_mb_9 + data_exclude_70.vol_3g_mb_9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating churn variable : those who have not used either calls or internet in month of September are customers who have churned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_exclude_70['churn'] = data_exclude_70.apply(lambda x:1 if (x.tot_inc_out_9 == 0 and x.tot_internet_9 == 0) else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_exclude_70 = data_exclude_70.drop(['tot_inc_out_9','tot_internet_9'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking churn percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round((data_exclude_70.churn.value_counts()/len(data_exclude_70))*100,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting columns that belong to churn month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_exclude_70 = data_exclude_70.filter(regex='[^9]$',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering out numerical columns\n",
    "data_exclude_70.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = data_exclude_70.select_dtypes(include='object')\n",
    "numerical_columns = data_exclude_70.select_dtypes(include=['int64','float64'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Boxplot function for univariate analysis \n",
    "def num_col_univariate_analysis(c):\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    ax = sns.boxplot(y=c, data=data_exclude_70)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Boxplot function for bivariate analysis\n",
    "def num_col_bivariate_analysis(c1,c2):\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    ax = sns.boxplot(x=c1, y=c2, data=data_exclude_70)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Univariate Analysis of Numerical Columns\n",
    "for c in numerical_columns:\n",
    "    num_col_univariate_analysis(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate for categorical columns\n",
    "plt.figure(figsize=(20,20))\n",
    "i=0\n",
    "j = 1\n",
    "while i < len(categorical_columns):\n",
    "    if '9' in categorical_columns[i]:\n",
    "        i+=1\n",
    "    else:\n",
    "        plt.subplot(3,3,j)\n",
    "        plt.title(categorical_columns[i])\n",
    "        sns.countplot(categorical_columns[i],data=data_exclude_70)\n",
    "        #print(categorical_columns[i])\n",
    "        i+=1\n",
    "        j+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate Analysis of Numerical Columns\n",
    "for c in numerical_columns:\n",
    "    num_col_bivariate_analysis('churn',c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the correlation between all set of usable columns\n",
    "plt.figure(figsize=(30, 15))\n",
    "sns.heatmap(data_exclude_70.corr(), cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "plt.figure(figsize=(200,200))\n",
    "while i < len(numerical_columns):\n",
    "    plt.subplot(13,13,i+1)\n",
    "    plt.title(numerical_columns[i])\n",
    "    sns.boxplot(y=numerical_columns[i],data=data_exclude_70_out)\n",
    "    i+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Derived Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_exclude_70['arpu_diff'] = data_exclude_70.arpu_8 - ((data_exclude_70.arpu_6 + data_exclude_70.arpu_7)/2)\n",
    "data_exclude_70['onnet_mou_diff'] = data_exclude_70.onnet_mou_8 - ((data_exclude_70.onnet_mou_6 + data_exclude_70.onnet_mou_7)/2)\n",
    "data_exclude_70['offnet_mou_diff'] = data_exclude_70.offnet_mou_8 - ((data_exclude_70.offnet_mou_6 + data_exclude_70.offnet_mou_7)/2)\n",
    "data_exclude_70['roam_ic_mou_diff'] = data_exclude_70.roam_ic_mou_8 - ((data_exclude_70.roam_ic_mou_6 + data_exclude_70.roam_ic_mou_7)/2)\n",
    "data_exclude_70['roam_og_mou_diff'] = data_exclude_70.roam_og_mou_8 - ((data_exclude_70.roam_og_mou_6 + data_exclude_70.roam_og_mou_7)/2)\n",
    "data_exclude_70['loc_og_mou_diff'] = data_exclude_70.loc_og_mou_8 - ((data_exclude_70.loc_og_mou_6 + data_exclude_70.loc_og_mou_7)/2)\n",
    "data_exclude_70['std_og_mou_diff'] = data_exclude_70.std_og_mou_8 - ((data_exclude_70.std_og_mou_6 + data_exclude_70.std_og_mou_7)/2)\n",
    "data_exclude_70['isd_og_mou_diff'] = data_exclude_70.isd_og_mou_8 - ((data_exclude_70.isd_og_mou_6 + data_exclude_70.isd_og_mou_7)/2)\n",
    "data_exclude_70['spl_og_mou_diff'] = data_exclude_70.spl_og_mou_8 - ((data_exclude_70.spl_og_mou_6 + data_exclude_70.spl_og_mou_7)/2)\n",
    "data_exclude_70['total_og_mou_diff'] = data_exclude_70.total_og_mou_8 - ((data_exclude_70.total_og_mou_6 + data_exclude_70.total_og_mou_7)/2)\n",
    "data_exclude_70['loc_ic_mou_diff'] = data_exclude_70.loc_ic_mou_8 - ((data_exclude_70.loc_ic_mou_6 + data_exclude_70.loc_ic_mou_7)/2)\n",
    "data_exclude_70['std_ic_mou_diff'] = data_exclude_70.std_ic_mou_8 - ((data_exclude_70.std_ic_mou_6 + data_exclude_70.std_ic_mou_7)/2)\n",
    "data_exclude_70['isd_ic_mou_diff'] = data_exclude_70.isd_ic_mou_8 - ((data_exclude_70.isd_ic_mou_6 + data_exclude_70.isd_ic_mou_7)/2)\n",
    "data_exclude_70['spl_ic_mou_diff'] = data_exclude_70.spl_ic_mou_8 - ((data_exclude_70.spl_ic_mou_6 + data_exclude_70.spl_ic_mou_7)/2)\n",
    "data_exclude_70['total_ic_mou_diff'] = data_exclude_70.total_ic_mou_8 - ((data_exclude_70.total_ic_mou_6 + data_exclude_70.total_ic_mou_7)/2)\n",
    "data_exclude_70['total_rech_num_diff'] = data_exclude_70.total_rech_num_8 - ((data_exclude_70.total_rech_num_6 + data_exclude_70.total_rech_num_7)/2)\n",
    "data_exclude_70['total_rech_amt_diff'] = data_exclude_70.total_rech_amt_8 - ((data_exclude_70.total_rech_amt_6 + data_exclude_70.total_rech_amt_7)/2)\n",
    "data_exclude_70['max_rech_amt_diff'] = data_exclude_70.max_rech_amt_8 - ((data_exclude_70.max_rech_amt_6 + data_exclude_70.max_rech_amt_7)/2)\n",
    "data_exclude_70['total_rech_data_diff'] = data_exclude_70.total_rech_data_8 - ((data_exclude_70.total_rech_data_6 + data_exclude_70.total_rech_data_7)/2)\n",
    "data_exclude_70['max_rech_data_diff'] = data_exclude_70.max_rech_data_8 - ((data_exclude_70.max_rech_data_6 + data_exclude_70.max_rech_data_7)/2)\n",
    "data_exclude_70['av_rech_amt_data_diff'] = data_exclude_70.av_rech_amt_data_8 - ((data_exclude_70.av_rech_amt_data_6 + data_exclude_70.av_rech_amt_data_7)/2)\n",
    "data_exclude_70['vol_2g_mb_diff'] = data_exclude_70.vol_2g_mb_8 - ((data_exclude_70.vol_2g_mb_6 + data_exclude_70.vol_2g_mb_7)/2)\n",
    "data_exclude_70['vol_3g_mb_diff'] = data_exclude_70.vol_3g_mb_8 - ((data_exclude_70.vol_3g_mb_6 + data_exclude_70.vol_3g_mb_7)/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Excluding the Outliers in the numerical columns\n",
    "\n",
    "data_exclude_70_out = data_exclude_70.copy()\n",
    "for i in numerical_columns:\n",
    "    UR = data_exclude_70_out[i].quantile(0.999)\n",
    "    LR = data_exclude_70_out[i].quantile(0.001)\n",
    "    data_exclude_70_out = data_exclude_70_out[(data_exclude_70_out[i] >= LR) & (data_exclude_70_out[i] <= UR)]\n",
    "data_exclude_70_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Creating dummy variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns =['night_pck_user_6',\n",
    " 'night_pck_user_7',\n",
    " 'night_pck_user_8',\n",
    " 'fb_user_6',\n",
    " 'fb_user_7',\n",
    " 'fb_user_8',\n",
    " ]\n",
    "\n",
    "data_exclude_dummy = pd.get_dummies(data_exclude_70_out[categorical_columns])\n",
    "data_exclude_70_drop = data_exclude_70_out.drop(categorical_columns,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_dummy = pd.concat([data_exclude_70_drop,data_exclude_dummy],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change churn to numeric\n",
    "data_with_dummy['churn'] = pd.to_numeric(data_with_dummy['churn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  8. Performing Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide data into train test split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = data_with_dummy.drop(\"churn\", axis = 1)\n",
    "y = data_with_dummy.churn\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 4, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print shapes of train and test sets\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 .Handling Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the churn rate\n",
    "round((data_exclude_70.churn.value_counts()/len(data_exclude_70))*100,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smt = SMOTE(random_state=45, k_neighbors=5)\n",
    "X_train_resampled_smt, y_train_resampled_smt = smt.fit_resample(X_train, y_train)\n",
    "X_train_resampled_smt.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 10. Scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the columns using StandardScaler on train dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "col = [i for i in X_train_resampled_smt.columns if i not in ['night_pck_user_6','night_pck_user_7','night_pck_user_8','fb_user_6','fb_user_7','fb_user_8']]\n",
    "scaler = StandardScaler()\n",
    "X_train_resampled_smt[col] = scaler.fit_transform(X_train_resampled_smt[col])\n",
    "X_train_resampled_smt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the columns using StandardScaler on test dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_test_scaled = X_test.copy()\n",
    "col = [i for i in X_test.columns if i not in ['night_pck_user_6','night_pck_user_7','night_pck_user_8','fb_user_6','fb_user_7','fb_user_8']]\n",
    "X_test_scaled[col] = scaler.transform(X_test_scaled[col])\n",
    "X_test_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model I : Logistic Regression without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the LogisticRegression object and predicting the results on test \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lreg = LogisticRegression()\n",
    "lreg.fit(X_train_resampled_smt, y_train_resampled_smt)\n",
    "y_pred = lreg.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting 15 features that describes the dependent variable the most\n",
    "logreg = LogisticRegression(random_state=100)\n",
    "rfe = RFE(logreg, 15)             # running RFE with 15 variables as output\n",
    "rfe = rfe.fit(X_train_resampled_smt, y_train_resampled_smt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing list of columns selected by the RFE\n",
    "rfe_selected_columns = pd.DataFrame(X_train_resampled_smt).columns[rfe.support_]\n",
    "rfe_selected_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at the statistical summary of the model created\n",
    "feature_list = ['onnet_mou_8', 'std_og_t2m_mou_8', 'total_og_mou_8',\n",
    "       'total_rech_data_6', 'total_rech_data_7', 'count_rech_2g_6',\n",
    "       'count_rech_2g_7', 'count_rech_3g_6', 'count_rech_3g_7', 'monthly_2g_7',\n",
    "       'sachet_2g_7', 'monthly_3g_6', 'monthly_3g_7', 'sachet_3g_6',\n",
    "       'sachet_3g_7']\n",
    "X_train_resampled_smt_sm = sm.add_constant(X_train_resampled_smt[rfe_selected_columns])\n",
    "logm = sm.GLM(y_train_resampled_smt,X_train_resampled_smt_sm, family = sm.families.Binomial())\n",
    "logm.fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing feature with high pvalues\n",
    "feature_list.remove('sachet_3g_6')\n",
    "X_train_resampled_smt_sm = sm.add_constant(X_train_resampled_smt[feature_list])\n",
    "logm = sm.GLM(y_train_resampled_smt,X_train_resampled_smt_sm, family = sm.families.Binomial())\n",
    "logm.fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing feature with high pvalues\n",
    "feature_list.remove('sachet_3g_7')\n",
    "X_train_resampled_smt_sm = sm.add_constant(X_train_resampled_smt[feature_list])\n",
    "logm = sm.GLM(y_train_resampled_smt,X_train_resampled_smt_sm, family = sm.families.Binomial())\n",
    "logm.fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing feature with high pvalues\n",
    "feature_list.remove('sachet_2g_7')\n",
    "X_train_resampled_smt_sm = sm.add_constant(X_train_resampled_smt[feature_list])\n",
    "logm = sm.GLM(y_train_resampled_smt,X_train_resampled_smt_sm, family = sm.families.Binomial())\n",
    "logm.fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing feature with high pvalues\n",
    "feature_list.remove('count_rech_3g_7')\n",
    "X_train_resampled_smt_sm = sm.add_constant(X_train_resampled_smt[feature_list])\n",
    "logm = sm.GLM(y_train_resampled_smt,X_train_resampled_smt_sm, family = sm.families.Binomial())\n",
    "logm.fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing feature with high pvalues\n",
    "feature_list.remove('count_rech_2g_6')\n",
    "X_train_resampled_smt_sm = sm.add_constant(X_train_resampled_smt[feature_list])\n",
    "logm = sm.GLM(y_train_resampled_smt,X_train_resampled_smt_sm, family = sm.families.Binomial())\n",
    "res = logm.fit().summary()\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train_resampled_smt[feature_list].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train_resampled_smt[feature_list].values, i) for i in range(X_train_resampled_smt[feature_list].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing feature with high VIF\n",
    "feature_list.remove('total_rech_data_7')\n",
    "X_train_resampled_smt_sm = sm.add_constant(X_train_resampled_smt[feature_list])\n",
    "logm = sm.GLM(y_train_resampled_smt,X_train_resampled_smt_sm, family = sm.families.Binomial())\n",
    "res = logm.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train_resampled_smt[feature_list].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train_resampled_smt[feature_list].values, i) for i in range(X_train_resampled_smt[feature_list].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing feature with high VIF\n",
    "feature_list.remove('total_og_mou_8')\n",
    "X_train_resampled_smt_sm = sm.add_constant(X_train_resampled_smt[feature_list])\n",
    "logm = sm.GLM(y_train_resampled_smt,X_train_resampled_smt_sm, family = sm.families.Binomial())\n",
    "res = logm.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing feature with high VIF\n",
    "feature_list.remove('total_rech_data_6')\n",
    "X_train_resampled_smt_sm = sm.add_constant(X_train_resampled_smt[feature_list])\n",
    "logm = sm.GLM(y_train_resampled_smt,X_train_resampled_smt_sm, family = sm.families.Binomial())\n",
    "res = logm.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train_resampled_smt[feature_list].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train_resampled_smt[feature_list].values, i) for i in range(X_train_resampled_smt[feature_list].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_resampled_smt_pred = res.predict(X_train_resampled_smt_sm).values.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the probability of churn for each customer\n",
    "y_train_pred_final = pd.DataFrame({'ActualChurn':y_train_resampled_smt.values, 'ChurnProbability':y_train_resampled_smt_pred})\n",
    "y_train_pred_final['Predicted'] = y_train_pred_final.ChurnProbability.map(lambda x: 1 if x > 0.5 else 0)\n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the overall accuracy.\n",
    "print(metrics.accuracy_score(y_train_pred_final.ActualChurn, y_train_pred_final.Predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(y_train_pred_final.ActualChurn, y_train_pred_final.Predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Accuracy: ', accuracy_score(y_train_pred_final.ActualChurn, y_train_pred_final.Predicted))\n",
    "print ('F1 score: ', f1_score(y_train_pred_final.ActualChurn, y_train_pred_final.Predicted))\n",
    "print ('Recall: ', recall_score(y_train_pred_final.ActualChurn, y_train_pred_final.Predicted))\n",
    "print ('Precision: ', precision_score(y_train_pred_final.ActualChurn, y_train_pred_final.Predicted))\n",
    "print ('\\n clasification report:\\n', classification_report(y_train_pred_final.ActualChurn, y_train_pred_final.Predicted))\n",
    "print ('\\n confussion matrix:\\n',confusion_matrix(y_train_pred_final.ActualChurn, y_train_pred_final.Predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_roc( actual, probs ):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(actual, probs, drop_intermediate = False)\n",
    "    auc_score = metrics.roc_auc_score(actual, probs)\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    return None\n",
    "draw_roc(y_train_pred_final.ActualChurn, y_train_pred_final.ChurnProbability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create columns with different probability cutoffs \n",
    "numbers = [float(x)/10 for x in range(10)]\n",
    "\n",
    "for i in numbers:\n",
    "    y_train_pred_final[i]= y_train_pred_final.ChurnProbability.map(lambda x: 1 if x > i else 0)\n",
    "    \n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy sensitivity and specificity for various probability cutoffs.\n",
    "cutoff_df = pd.DataFrame( columns = ['CutOff_Probability','Accuracy','Sensitivity','Specificity'])\n",
    "\n",
    "# TP = confusion[1,1] # true positive \n",
    "# TN = confusion[0,0] # true negatives\n",
    "# FP = confusion[0,1] # false positives\n",
    "# FN = confusion[1,0] # false negatives\n",
    "\n",
    "num = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "for i in num:\n",
    "    cm1 = metrics.confusion_matrix(y_train_pred_final.ActualChurn, y_train_pred_final[i])\n",
    "    total1=sum(sum(cm1))\n",
    "    Accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
    "    \n",
    "    Specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    Sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    cutoff_df.loc[i] =[ i ,Accuracy,Sensitivity,Specificity]\n",
    "    \n",
    "print(cutoff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy sensitivity and specificity for various probabilities.\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(cutoff_df['CutOff_Probability'], cutoff_df['Accuracy'], label='Accuracy')\n",
    "plt.plot(cutoff_df['CutOff_Probability'], cutoff_df['Sensitivity'], label='Sensitivity')\n",
    "plt.plot(cutoff_df['CutOff_Probability'], cutoff_df['Specificity'], label='Specificity')\n",
    "plt.xlabel('Probability Cutoff')\n",
    "plt.title('Probability Cutoff vs Accuracy, Sensitivity and Specificity')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign label based on optimul cutoff probability\n",
    "y_train_pred_final['final_predicted'] = y_train_pred_final.ChurnProbability.map( lambda x: 1 if x > .58 else 0)\n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the overall accuracy.\n",
    "metrics.accuracy_score(y_train_pred_final.ActualChurn, y_train_pred_final.final_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Accuracy: ', accuracy_score(y_train_pred_final.ActualChurn, y_train_pred_final.Predicted))\n",
    "print ('F1 score: ', f1_score(y_train_pred_final.ActualChurn, y_train_pred_final.Predicted))\n",
    "print ('Recall: ', recall_score(y_train_pred_final.ActualChurn, y_train_pred_final.Predicted))\n",
    "print ('Precision: ', precision_score(y_train_pred_final.ActualChurn, y_train_pred_final.Predicted))\n",
    "print ('\\n clasification report:\\n', classification_report(y_train_pred_final.ActualChurn, y_train_pred_final.Predicted))\n",
    "print ('\\n confussion matrix:\\n',confusion_matrix(y_train_pred_final.ActualChurn, y_train_pred_final.Predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting on test dataset\n",
    "X_test_new = X_test_scaled[feature_list]\n",
    "X_test_sm = sm.add_constant(X_test_new)\n",
    "\n",
    "y_test_pred = res.predict(X_test_sm).values.reshape(-1)\n",
    "\n",
    "# Converting y_pred to a dataframe which is an array\n",
    "y_pred_1 = pd.DataFrame(y_test_pred)\n",
    "\n",
    "# Converting y_test to dataframe\n",
    "y_test_df = pd.DataFrame(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing index for both dataframes to append them side by side \n",
    "y_pred_1.reset_index(drop=True, inplace=True)\n",
    "y_test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Appending y_test_df and y_pred_1\n",
    "y_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)\n",
    "y_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the column \n",
    "y_pred_final= y_pred_final.rename(columns={ 0 : 'Churn_Prob'})\n",
    "y_pred_final['final_predicted'] = y_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.58 else 0)\n",
    "y_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the overall accuracy.\n",
    "metrics.accuracy_score(y_pred_final.churn, y_pred_final.final_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Dataset :')\n",
    "print ('Accuracy: ', accuracy_score(y_pred_final.churn, y_pred_final.final_predicted))\n",
    "print ('F1 score: ', f1_score(y_pred_final.churn, y_pred_final.final_predicted))\n",
    "print ('Recall: ', recall_score(y_pred_final.churn, y_pred_final.final_predicted))\n",
    "print ('Precision: ', precision_score(y_pred_final.churn, y_pred_final.final_predicted))\n",
    "print ('\\n clasification report:\\n', classification_report(y_pred_final.churn, y_pred_final.final_predicted))\n",
    "print ('\\n confussion matrix:\\n',confusion_matrix(y_pred_final.churn, y_pred_final.final_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could see that the model performance and the metrics were low. So we will further implement PCA with Logistic Regression and see what results we get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model II : PCA and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# apply pca to train data\n",
    "pca = Pipeline([('pca', PCA())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the model on training dataset\n",
    "pca.fit(X_train_resampled_smt)\n",
    "churn_pca = pca.fit_transform(X_train_resampled_smt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract pca model from pipeline\n",
    "pca = pca.named_steps['pca']\n",
    "\n",
    "# look at explainded variance of PCA components\n",
    "print(pd.Series(np.round(pca.explained_variance_ratio_.cumsum(), 4)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_cumu = np.cumsum(pca.explained_variance_ratio_)\n",
    "fig = plt.figure(figsize=[12,8])\n",
    "plt.vlines(x=15, ymax=1, ymin=0, colors=\"r\", linestyles=\"--\")\n",
    "plt.hlines(y=0.95, xmax=30, xmin=0, colors=\"g\", linestyles=\"--\")\n",
    "plt.plot(var_cumu)\n",
    "plt.ylabel(\"Cumulative variance explained\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "PCA_VARS = 60\n",
    "steps = [(\"pca\", PCA(n_components=PCA_VARS)),\n",
    "         (\"logistic\", LogisticRegression(class_weight='balanced'))\n",
    "        ]\n",
    "pipeline = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "pipeline.fit(X_train_resampled_smt, y_train_resampled_smt)\n",
    "\n",
    "# check score on train data\n",
    "pipeline.score(X_train_resampled_smt, y_train_resampled_smt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict churn on test data\n",
    "y_pred = pipeline.predict(X_test_scaled)\n",
    "\n",
    "# create onfusion matrix\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "print(confusion)\n",
    "\n",
    "# check sensitivity and specificity\n",
    "TP = confusion[1,1] # true positive \n",
    "TN = confusion[0,0] # true negatives\n",
    "FP = confusion[0,1] # false positives\n",
    "FN = confusion[1,0] # false negatives\n",
    "\n",
    "# Calculate sensitivity\n",
    "sensitivity = TP / float(TP+FN)\n",
    "\n",
    "# Calculate specificity\n",
    "specificity = TN / float(TN+FP)\n",
    "print(\"Sensitivity: \\t\", round(sensitivity, 2), \"\\n\", \"Specificity: \\t\", round(specificity, 2), sep='')\n",
    "\n",
    "# check area under curve\n",
    "y_pred_prob = pipeline.predict_proba(X_test)[:, 1]\n",
    "print(\"AUC:    \\t\", round(roc_auc_score(y_test, y_pred_prob),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Dataset :')\n",
    "print ('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print ('F1 score: ', f1_score(y_test, y_pred))\n",
    "print ('Recall: ', recall_score(y_test, y_pred))\n",
    "print ('Precision: ', precision_score(y_test, y_pred))\n",
    "print ('\\n clasification report:\\n', classification_report(y_test,y_pred))\n",
    "print ('\\n confussion matrix:\\n',confusion_matrix(y_test, y_pred))\n",
    "confusion = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning - PCA and logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class imbalance\n",
    "y_train.value_counts()/y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "pca = PCA(random_state=42)\n",
    "\n",
    "# logistic regression - the class weight is used to handle class imbalance - it adjusts the cost function\n",
    "logistic = LogisticRegression()#class_weight={0:0.1, 1: 0.9})\n",
    "\n",
    "# create pipeline\n",
    "steps = [ \n",
    "         (\"pca\", pca),\n",
    "         (\"logistic\", logistic)\n",
    "        ]\n",
    "\n",
    "# compile pipeline\n",
    "pca_logistic = Pipeline(steps)\n",
    "\n",
    "# hyperparameter space\n",
    "params = {'pca__n_components': [50,60, 80,90,100], 'logistic__C': [0.001,0.01,0.1, 0.5, 1, 2, 3, 4, 5, 10], 'logistic__penalty': ['l1', 'l2']}\n",
    "\n",
    "# create 5 folds\n",
    "folds = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 4)\n",
    "\n",
    "# create gridsearch object\n",
    "model = GridSearchCV(estimator=pca_logistic, cv=folds, param_grid=params, scoring='roc_auc', n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "model.fit(X_train_resampled_smt, y_train_resampled_smt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation results\n",
    "pd.DataFrame(model.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print best hyperparameters\n",
    "print(\"Best AUC: \", model.best_score_)\n",
    "print(\"Best hyperparameters: \", model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "pca = PCA(random_state=42)\n",
    "\n",
    "# logistic regression - the class weight is used to handle class imbalance - it adjusts the cost function\n",
    "logistic = LogisticRegression()#class_weight={0:0.1, 1: 0.9})\n",
    "\n",
    "# create pipeline\n",
    "steps = [ \n",
    "         (\"pca\", pca),\n",
    "         (\"logistic\", logistic)\n",
    "        ]\n",
    "\n",
    "# compile pipeline\n",
    "pca_logistic = Pipeline(steps)\n",
    "\n",
    "# hyperparameter space\n",
    "params = {'pca__n_components': [100], 'logistic__C': [10], 'logistic__penalty': ['l2']}\n",
    "\n",
    "# create 5 folds\n",
    "folds = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 4)\n",
    "\n",
    "# create gridsearch object\n",
    "model = GridSearchCV(estimator=pca_logistic, cv=folds, param_grid=params, scoring='roc_auc', n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "model.fit(X_train_resampled_smt, y_train_resampled_smt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating on test data\n",
    "y_pred = pipeline.predict(X_test_scaled)\n",
    "\n",
    "# create onfusion matrix\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "print(confusion)\n",
    "\n",
    "# check sensitivity and specificity\n",
    "TP = confusion[1,1] # true positive \n",
    "TN = confusion[0,0] # true negatives\n",
    "FP = confusion[0,1] # false positives\n",
    "FN = confusion[1,0] # false negatives\n",
    "\n",
    "# Calculate sensitivity\n",
    "sensitivity = TP / float(TP+FN)\n",
    "\n",
    "# Calculate specificity\n",
    "specificity = TN / float(TN+FP)\n",
    "print(\"Sensitivity: \\t\", round(sensitivity, 2), \"\\n\", \"Specificity: \\t\", round(specificity, 2), sep='')\n",
    "\n",
    "# check area under curve\n",
    "y_pred_prob = pipeline.predict_proba(X_test)[:, 1]\n",
    "print(\"AUC:    \\t\", round(roc_auc_score(y_test, y_pred_prob),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Dataset :')\n",
    "print ('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print ('F1 score: ', f1_score(y_test, y_pred))\n",
    "print ('Recall: ', recall_score(y_test, y_pred))\n",
    "print ('Precision: ', precision_score(y_test, y_pred))\n",
    "print ('\\n clasification report:\\n', classification_report(y_test,y_pred))\n",
    "print ('\\n confussion matrix:\\n',confusion_matrix(y_test, y_pred))\n",
    "confusion = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model III : Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# PCA\n",
    "pca = PCA(random_state=42)\n",
    "decision_tree = DecisionTreeClassifier()#class_weight={0:0.1, 1: 0.9})\n",
    "\n",
    "# create pipeline\n",
    "steps = [ \n",
    "         (\"pca\", pca),\n",
    "         (\"decisiontreeclassifier\", decision_tree)\n",
    "        ]\n",
    "\n",
    "# compile pipeline\n",
    "pca_dt = Pipeline(steps)\n",
    "\n",
    "# hyperparameter space\n",
    "params = {'pca__n_components': [50,60,80,90,100,120],'decisiontreeclassifier__max_depth':[3,4,5],\n",
    "              'decisiontreeclassifier__min_samples_leaf':[10,20,30]}\n",
    "\n",
    "# create 5 folds\n",
    "folds = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 4)\n",
    "\n",
    "# create gridsearch object\n",
    "model = GridSearchCV(estimator=pca_dt, cv=folds, param_grid=params, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_resampled_smt, y_train_resampled_smt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print best hyperparameters\n",
    "print(\"Best AUC: \", model.best_score_)\n",
    "print(\"Best hyperparameters: \", model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# PCA\n",
    "pca = PCA(random_state=42)\n",
    "decision_tree = DecisionTreeClassifier()#class_weight={0:0.1, 1: 0.9})\n",
    "\n",
    "# create pipeline\n",
    "steps = [ \n",
    "         (\"pca\", pca),\n",
    "         (\"decisiontreeclassifier\", decision_tree)\n",
    "        ]\n",
    "\n",
    "# compile pipeline\n",
    "pca_dt = Pipeline(steps)\n",
    "\n",
    "# hyperparameter space\n",
    "params = {'pca__n_components': [120],'decisiontreeclassifier__max_depth':[5],\n",
    "              'decisiontreeclassifier__min_samples_leaf':[10]}\n",
    "\n",
    "# create 5 folds\n",
    "folds = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 4)\n",
    "\n",
    "# create gridsearch object\n",
    "model = GridSearchCV(estimator=pca_dt, cv=folds, param_grid=params, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating on test data\n",
    "\n",
    "# predict churn on test data\n",
    "y_pred = pipeline.predict(X_test_scaled)\n",
    "\n",
    "# create onfusion matrix\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "print(confusion)\n",
    "\n",
    "# check sensitivity and specificity\n",
    "TP = confusion[1,1] # true positive \n",
    "TN = confusion[0,0] # true negatives\n",
    "FP = confusion[0,1] # false positives\n",
    "FN = confusion[1,0] # false negatives\n",
    "\n",
    "# Calculate sensitivity\n",
    "sensitivity = TP / float(TP+FN)\n",
    "\n",
    "# Calculate specificity\n",
    "specificity = TN / float(TN+FP)\n",
    "print(\"Sensitivity: \\t\", round(sensitivity, 2), \"\\n\", \"Specificity: \\t\", round(specificity, 2), sep='')\n",
    "\n",
    "# check area under curve\n",
    "y_pred_prob = pipeline.predict_proba(X_test)[:, 1]\n",
    "print(\"AUC:    \\t\", round(roc_auc_score(y_test, y_pred_prob),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Dataset :')\n",
    "print ('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print ('F1 score: ', f1_score(y_test, y_pred))\n",
    "print ('Recall: ', recall_score(y_test, y_pred))\n",
    "print ('Precision: ', precision_score(y_test, y_pred))\n",
    "print ('\\n clasification report:\\n', classification_report(y_test,y_pred))\n",
    "print ('\\n confussion matrix:\\n',confusion_matrix(y_test, y_pred))\n",
    "confusion = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model IV : Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# PCA\n",
    "pca = PCA(random_state=42)\n",
    "random_forest = RandomForestClassifier()#class_weight={0:0.1, 1: 0.9})\n",
    "\n",
    "# create pipeline\n",
    "steps = [ \n",
    "         (\"pca\", pca),\n",
    "         (\"RandomForestClassifier\", random_forest)\n",
    "        ]\n",
    "\n",
    "# compile pipeline\n",
    "pca_dt = Pipeline(steps)\n",
    "\n",
    "# hyperparameter space\n",
    "params = {'pca__n_components': [60,80,100,120],'RandomForestClassifier__max_depth':[3,4,5],\n",
    "              'RandomForestClassifier__min_samples_leaf':[10,20,30],'RandomForestClassifier__n_estimators':[10,20,30,40]}\n",
    "\n",
    "# create 5 folds\n",
    "folds = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 4)\n",
    "\n",
    "# create gridsearch object\n",
    "model = GridSearchCV(estimator=pca_dt, cv=folds, param_grid=params, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_resampled_smt, y_train_resampled_smt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print best hyperparameters\n",
    "print(\"Best AUC: \", model.best_score_)\n",
    "print(\"Best hyperparameters: \", model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# PCA\n",
    "pca = PCA(random_state=42)\n",
    "random_forest = RandomForestClassifier()#class_weight={0:0.1, 1: 0.9})\n",
    "\n",
    "# create pipeline\n",
    "steps = [ \n",
    "         (\"pca\", pca),\n",
    "         (\"RandomForestClassifier\", random_forest)\n",
    "        ]\n",
    "\n",
    "# compile pipeline\n",
    "pca_dt = Pipeline(steps)\n",
    "\n",
    "# hyperparameter space\n",
    "params = {'pca__n_components': [80],'RandomForestClassifier__max_depth':[5],\n",
    "              'RandomForestClassifier__min_samples_leaf':[20],'RandomForestClassifier__n_estimators':[20]}\n",
    "\n",
    "# create 5 folds\n",
    "folds = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 4)\n",
    "\n",
    "# create gridsearch object\n",
    "model = GridSearchCV(estimator=pca_dt, cv=folds, param_grid=params, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating on test data\n",
    "# predict churn on test data\n",
    "y_pred = pipeline.predict(X_test_scaled)\n",
    "\n",
    "# create onfusion matrix\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "print(confusion)\n",
    "\n",
    "# check sensitivity and specificity\n",
    "TP = confusion[1,1] # true positive \n",
    "TN = confusion[0,0] # true negatives\n",
    "FP = confusion[0,1] # false positives\n",
    "FN = confusion[1,0] # false negatives\n",
    "\n",
    "# Calculate sensitivity\n",
    "sensitivity = TP / float(TP+FN)\n",
    "\n",
    "# Calculate specificity\n",
    "specificity = TN / float(TN+FP)\n",
    "print(\"Sensitivity: \\t\", round(sensitivity, 2), \"\\n\", \"Specificity: \\t\", round(specificity, 2), sep='')\n",
    "\n",
    "# check area under curve\n",
    "y_pred_prob = pipeline.predict_proba(X_test_scaled)[:, 1]\n",
    "print(\"AUC:    \\t\", round(roc_auc_score(y_test, y_pred_prob),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Dataset :')\n",
    "print ('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print ('F1 score: ', f1_score(y_test, y_pred))\n",
    "print ('Recall: ', recall_score(y_test, y_pred))\n",
    "print ('Precision: ', precision_score(y_test, y_pred))\n",
    "print ('\\n clasification report:\\n', classification_report(y_test,y_pred))\n",
    "print ('\\n confussion matrix:\\n',confusion_matrix(y_test, y_pred))\n",
    "confusion = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a random forest model on train data\n",
    "max_features = int(round(np.sqrt(X_train.shape[1])))    # number of variables to consider to split each node\n",
    "print(max_features)\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_features=max_features, class_weight={0:0.1, 1: 0.9}, oob_score=True, random_state=4, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOB score\n",
    "rf_model.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict churn on test data\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "#Confusion Matrix\n",
    "TP = confusion[1,1] # true positive \n",
    "TN = confusion[0,0] # true negatives\n",
    "FP = confusion[0,1] # false positives\n",
    "FN = confusion[1,0] # false negatives\n",
    "\n",
    "# Calculate sensitivity\n",
    "print('Sensitivity:',TP / float(TP+FN))\n",
    "\n",
    "# Calculate specificity\n",
    "print('Specificity:',TN / float(TN+FP))\n",
    "\n",
    "# Calculate false postive rate - predicting Converted when customer does not have Converted\n",
    "print('False Positive Rate:',FP/ float(TN+FP))\n",
    "\n",
    "# positive predictive value \n",
    "print('Positive Predictive Value:',TP / float(TP+FP))\n",
    "\n",
    "# Negative predictive value\n",
    "print('Negative Predictive Value:',TN / float(TN+ FN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Test Dataset :')\n",
    "print ('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print ('F1 score: ', f1_score(y_test, y_pred))\n",
    "print ('Recall: ', recall_score(y_test, y_pred))\n",
    "print ('Precision: ', precision_score(y_test, y_pred))\n",
    "print ('\\n clasification report:\\n', classification_report(y_test,y_pred))\n",
    "print ('\\n confussion matrix:\\n',confusion_matrix(y_test, y_pred))\n",
    "confusion = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictors\n",
    "features = data_exclude_70.drop('churn', axis=1).columns\n",
    "\n",
    "# feature_importance\n",
    "importance = rf_model.feature_importances_\n",
    "\n",
    "# create dataframe\n",
    "feature_importance = pd.DataFrame({'variables': features, 'importance_percentage': importance*100})\n",
    "feature_importance = feature_importance[['variables', 'importance_percentage']]\n",
    "\n",
    "# sort features\n",
    "feature_importance = feature_importance.sort_values('importance_percentage', ascending=False).reset_index(drop=True)\n",
    "print(\"Sum of importance=\", feature_importance.importance_percentage.sum())\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract top 'n' features\n",
    "top_n = 30\n",
    "top_features = feature_importance.variables[0:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature correlation\n",
    "import seaborn as sns\n",
    "plt.rcParams[\"figure.figsize\"] =(10,10)\n",
    "mycmap = sns.diverging_palette(199, 359, s=99, center=\"light\", as_cmap=True)\n",
    "sns.heatmap(data=X_train[top_features].corr(), center=0.0, cmap=mycmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = ['total_ic_mou_8', 'total_rech_amt_diff', 'total_og_mou_8', 'arpu_8', 'roam_ic_mou_8', 'roam_og_mou_8', \n",
    "                'std_ic_mou_8', 'av_rech_amt_data_8', 'std_og_mou_8']\n",
    "X_train = X_train[top_features]\n",
    "X_test = X_test[top_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting intercept and slope from logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model = model.best_estimator_.named_steps['logistic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intercept\n",
    "intercept_df = pd.DataFrame(logistic_model.intercept_.reshape((1,1)), columns = ['intercept'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coefficients\n",
    "coefficients = logistic_model.coef_.reshape((9, 1)).tolist()\n",
    "coefficients = [val for sublist in coefficients for val in sublist]\n",
    "coefficients = [round(coefficient, 3) for coefficient in coefficients]\n",
    "\n",
    "logistic_features = list(X_train.columns)\n",
    "coefficients_df = pd.DataFrame(logistic_model.coef_, columns=logistic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate dataframes\n",
    "coefficients = pd.concat([intercept_df, coefficients_df], axis=1)\n",
    "coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The business should :\n",
    "    -\n",
    "    -\n",
    "    -\n",
    "    -"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
